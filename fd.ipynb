{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          post_title   post_date  post_score  \\\n",
      "0  Knorr has been reducing the quantity and incre...  2024-08-12         225   \n",
      "1  Knorr has been reducing the quantity and incre...  2024-08-12         225   \n",
      "2  Knorr has been reducing the quantity and incre...  2024-08-12         225   \n",
      "3  Knorr has been reducing the quantity and incre...  2024-08-12         225   \n",
      "4  Knorr has been reducing the quantity and incre...  2024-08-12         225   \n",
      "\n",
      "       post_author  post_num_comments comment_id     comment_author  \\\n",
      "0  Middle_child496                138    lhqgv0j       honest_jamal   \n",
      "1  Middle_child496                138    lhqgu8p            Poodina   \n",
      "2  Middle_child496                138    lhqi6r7  Stock-Respond5598   \n",
      "3  Middle_child496                138    lhqrh0r          kingshuk3   \n",
      "4  Middle_child496                138    lhqsbra           Zacnocap   \n",
      "\n",
      "                                        comment_body  comment_score  \\\n",
      "0  Indomie taste better, give more noodles and ar...             91   \n",
      "1  I dont understand why people aren't talking ab...             64   \n",
      "2                       Shrinkflation at its finest.             31   \n",
      "3                       Bro discovered shrinkflation             20   \n",
      "4  Indomie are faaar better then these , indomie ...             15   \n",
      "\n",
      "  comment_date  is_submitter  \n",
      "0   2024-08-12         False  \n",
      "1   2024-08-12         False  \n",
      "2   2024-08-12         False  \n",
      "3   2024-08-12         False  \n",
      "4   2024-08-12         False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Raman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(r'E:\\UTH\\reddit\\reddit_comments_20250130_003525.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_category\n",
      "positive    282\n",
      "neutral     277\n",
      "negative    121\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initialize the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment scores\n",
    "def get_sentiment(text):\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "# Apply sentiment analysis to each comment\n",
    "df['sentiment'] = df['comment_body'].apply(get_sentiment)\n",
    "\n",
    "# Classify sentiment as positive, neutral, or negative\n",
    "df['sentiment_category'] = df['sentiment'].apply(lambda x: 'positive' if x > 0.05 else ('negative' if x < -0.05 else 'neutral'))\n",
    "\n",
    "# Display the sentiment distribution\n",
    "print(df['sentiment_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "buy try bruh knorr lemon twist discontinued shrinkflation deleted miss\n",
      "Topic 1:\n",
      "removed sure chicken best ones maggie noodles good flavour knorr\n",
      "Topic 2:\n",
      "noodles knorr better indomie good maggi like taste just brand\n",
      "Topic 3:\n",
      "moderators pakistan imtiaz accounts thank action questions automatically contact concerns\n",
      "Topic 4:\n",
      "indomie flavor chin chow taste love shoop bro try better\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(df['comment_body'])\n",
    "\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=4709852)\n",
    "lda.fit(tfidf)\n",
    "\n",
    "# Display the top words for each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# Get the feature names\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the top 10 words for each topic\n",
    "display_topics(lda, tfidf_feature_names, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UTH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
